# LLM Classification Benchmark ğŸš€

This repository provides an **automated benchmarking pipeline** to evaluate Large Language Models (LLMs) on **popular text classification datasets** using the OpenAI API.

The framework is designed to be:
- âœ… Fully automated
- âš¡ Asynchronous (fast & scalable)
- ğŸ§  Dataset-agnostic
- ğŸ“Š Accuracy-focused

---

## ğŸ” What This Project Does

For each dataset, the script:
1. Automatically downloads the dataset from Hugging Face
2. Detects the input text and label columns
3. Samples a fixed number of examples
4. Dynamically generates a task-specific system prompt
5. Runs multiple LLMs asynchronously
6. Computes classification accuracy
7. Prints consolidated benchmark results

No manual preprocessing. No dataset-specific hacks.

---

## ğŸ“š Supported Task Types

- Binary sentiment classification
- Multi-class topic classification
- Emotion classification
- Intent classification
- Natural Language Inference (NLI)

âš ï¸ QA and RAG-style datasets (e.g., HotpotQA) are intentionally excluded.

---

## ğŸ§ª Benchmarked Datasets

| Dataset | Task Type |
|-------|----------|
| `stanfordnlp/imdb` | Movie sentiment (binary) |
| `ag_news` | News topic classification |
| `yelp_polarity` | Review sentiment |
| `dbpedia_14` | Entity/topic classification |
| `dair-ai/emotion` | Emotion classification |
| `amazon_polarity` | Product review sentiment |
| `banking77` | Intent classification |
| `snli` | Natural Language Inference |
| `yahoo_answers_topics` | Question topic classification |

All datasets are:
- Public
- Classification-only
- Plug-and-play via Hugging Face

---

## ğŸ§  Models Evaluated

| Logical Name | Model ID |
|-------------|----------|
| gpt-5.2 | gpt-5.2 |
| gpt-5-mini | gpt-5-mini |

(Replace model IDs if needed.)

---

## âš™ï¸ How It Works (High Level)

```text
Dataset â†’ Sample â†’ Prompt â†’ Async API Calls â†’ Accuracy â†’ Results
